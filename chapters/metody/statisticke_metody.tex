\subsection{Lineární smíšené modely}
\label{subsec:lmm}
Pro hodnocení dat z mise DIANA bylo využito \gls{LMM}. Tyto modely představují
flexibilní statistický nástroj pro analýzu dat s korelovanými nebo vnořenými
strukturami. Jedná se o rozšíření lineárních regresních modelů, které umožňují
zahrnutí jak fixních, tak náhodných efektů.

Základní myšlenkou za LMM je modelování vztahu mezi závislou proměnnou $Y$ a
jedním nebo více nezávislými proměnnými $X$, zatímco bere v úvahu skutečnost, že
data mohou mít hierarchickou strukturu, s pozorováními vnořenými v různých
skupinách. To se provádí specifikací lineární rovnice, která zahrnuje jak fixní,
tak náhodné efekty (resp. vektory těchto veličin):
\begin{equation}
    Y = \beta_0 + \sum \beta_i X_i + u + \epsilon
\end{equation}
kde $\beta$ je vektor koeficientů pro fixní efekty, $u$ je vektor náhodných
efektů a $\epsilon$ je chybový člen. Předpokládá se, že náhodný efekt $u$ se
řídí normálním rozdělením se střední hodnotou nula a rozptylem $\sigma_u^2$ a že
je nezávislý na chybovém členu $\epsilon$, který se rovněž řídí normálním
rozdělením se střední hodnotou nula a rozptylem $\sigma_\epsilon^2$.

V případě například pozorování závislé proměnné $Y_{ij}$ pro $i$-té pozorování v
$j$-té skupině bude náhodný efekt $u$ představovat variabilitu mezi skupinami a
je do modelu zahrnut proto, aby zohlednil skutečnost, že pozorování v rámci
jedné skupiny si budou pravděpodobně podobnější než pozorování v jiných
skupinách. Problematika \gls{LMM} je každopádně velmi obsáhlá a překračuje
rozsahem tuto práci. Rozebrána byla podrobně v literatuře~\cite{West2022}.

\subsection{Grangerova kauzalita}
\label{subsec:granger}
Grangerova kauzalita byla v této práci použita k hodnocení příčinných vztahů v
rámci fyziologických signálů a k zachycení jejich interakcí v závislosti na
čase. Grangerovo pojetí vychází z myšlenky, že příčina by měla být nápomocná při
předpovídání budoucích vlivů, a to nad rámec toho, co lze předpovědět pouze na
základě jejich vlastních minulých hodnot~\cite{Granger1969}.

Formálně, časová řada $X$ je nazvána \enquote{Grangerovou kauzalitou} jiné
časové řady $Y$, jestliže regrese pro $Y$ z hlediska minulých hodnot $Y$ a $X$
je statisticky významně přesnější než regrese pouze minulých hodnot $Y$. Nechť
$\{x_t\}_{t=1}^T$ jsou zpožděné vzorky řady $X$ a $\{y_t\}_{t=1}^T$
řady $Y$ (dále jen jako vektory $\overrightarrow{x_t}$ a $\overrightarrow{y_t}$).
Poté je prvním krokem Grangerova testu následující regrese~\cite{Arnold2007}:
\begin{equation}
    \begin{gathered}
        y_t \approx A \cdot y_{t-1}+B \cdot x_{t-1}^{\vec{t}} \\
        y_t \approx A \cdot y_{t-1}
    \end{gathered}
\end{equation}
po které je možné aplikovat různé statistické testy pro získání p-hodnoty a
následně je možné rozhodnout o výše zmíněné statistický významné přesnosti.

Běžně se Grangerova kauzalita aplikuje v rámci modelování časových řad ve smyslu
kombinatorického testování všech příznaků za účelem konstrukce výstupního
příznakového kauzálního grafu (resp. kauzální matice, viz
sekce~\ref{sec:hybridni_detekce}). Takové řešení by ale pro poměrně velký počet
fyziologických příznaků bylo extrémně výpočetně náročné, a proto bylo dále
uznáno za nevhodné.

Řešením se zde naskytla regrese, kterou lze využít pro identifikaci podmnožiny
příznaků, na které je daný příznak podmíněně závislý. To vychází z faktu, že
nejlepší regresor pro danou proměnnou s nejmenší kvadratickou chybou bude mít
teoreticky nenulové koeficienty pouze pro proměnné v okolí\footnote{Statisticky
    \enquote{okolí} proměnné implikuje podmnožinu proměnných, které s ní úzce
    souvisejí.}~\cite{Schindler2013,Arnold2007}. Pro tento regresní problém byl
zvolen Lasso algoritmus, jenž je uveden do souvislosti v následující sekci.

\subsection{Lasso regrese}
\label{subsec:lasso}
Lasso (Least Absolute Shrinkage and Selection Operator) je široce
používaná technika lineární regrese pro výběr a regularizaci proměnných využitím
$\ell_1$ penalizačního členu. Formálně, výstup $\vec{w}$ minimalizuje součet
průměrné kvadratické chyby regrese pro $y$:
\begin{equation}
    \vec{w}=\arg \min \frac{1}{n} \sum_{(\vec{x}, y) \in X}|\vec{w} \cdot \vec{x}-y|^2+\lambda\|\vec{w}\|_1
\end{equation}
kde $X$ je vstupní příznak, $n$ je počet vzorků v $X$ a $\lambda$ je penalizační
člen určující míru regularizace koeficientů. S rostoucí hodnotou $\lambda$ se
více koeficientů smršťuje směrem k nule, což vede k řídkému modelu s menším
počtem prediktorů a naopak~\cite{Tibshirani1996}. Ve smyslu tvorby kauzálních
matic poskytuje Lasso množinu časových proměnných, které při regresi $y_t$ podle
zpožděných proměnných $x_{t'}$, kde $t'=\{t-T,...,t -1\}$ pro všechna $x \in X$,
nabývají právě nenulového koeficientu Grangerovy kauzality.

Nicméně Bahadori a Liu dokázali v~\cite{Bahadori2013}, že Grangerova kauzalita
je v rámci použití vícerozměrných dat inkonzistentní a není dobře schopna
zachytit nelineární vztahy nebo složité struktury závislostí. Vzhledem k tomu,
že data využívaná v této práci jsou vícerozměrná a reprezentují biosignály, v
rámci kterých mohou existovat právě nelineární vztahy nebo komplexní
hierarchické relace, byl využit přístup Kopula-Granger. S využitím Lasso ukázali
v~\cite{Bahadori2013} jeho konzistenci na vícerozměrných datech i jeho schopnost
efektivně zachytit nelinearitu v datech (viz
sekce~\ref{subsec:kauzalni_matice}).

\subsection{Teorie kopulí}
\label{subsec:teorie_kopul}
Vzhledem k využití Kopula-Granger přístupu pro tvorbu vícerozměrných kauzálních
matic je žádoucí stručně představit kopula funkce. V teorii pravděpodobnosti a
statistice je kopule $C$ vícerozměrnou kumulativní distribuční funkcí, jež
popisuje závislost mezi jednotlivými marginálními distribucemi a poskytuje
způsob, jak modelovat právě společnou distribuci náhodných veličin bez
specifikace samotných distribucí těchto veličin. Formálně, je
$C:[0,1]^{d}\rightarrow [0,1]$ $d$-dimenzionální kopulou, pokud je $C$ sdílenou
\gls{CDF} $d$-dimenzionálního náhodného vektoru na jednotkové krychli
$[0,1]^{d}$ s rovnoměrnými marginály. Základem teorie kopulí je Sklarův teorém,
který říká, že jakákoli více-dimenzionální distribuce může být zapsána jako
kopule aplikovaná na její marginální distribuce:

\begin{theorem}[Sklarův teorém]
    \label{theorem:sklar}
    Nechť $F$ je vícerozměrná distribuční funkce s marginálními distribucemi
    $F_1, F_2, \ldots, F_n$. Pak existuje kopule $C$ taková, že:
    \begin{equation}
        F\left(x_1, x_2, \ldots, x_n\right)=C\left(F_1\left(x_1\right), F_2\left(x_2\right), \ldots, F_n\left(x_n\right)\right)
    \end{equation}
    Kopule $C$ je jednoznačná, pokud marginální distribuce $F_1, F_2, \ldots, F_n$ jsou spojité.
\end{theorem}

Kopula-Granger model (resp. Grangerův Non-paranormální model, G-NPN), v rámci
kterého jsou v této práci kopula funkcí mapovány marginální distribuce na úrovni
fyziologických událostí do kopula prostoru, je popsán v
sekci~\ref{subsec:kauzalni_matice}. Pro shrnutí vychází tento model z
následujících kroků~\cite{Guy2016}:
\begin{enumerate}
    \item Nalezení empirického marginálního rozdělení pro fyziologickou událost
          $\hat{F_i}$.
    \item Mapování pozorování do kopula prostoru:
          $\hat{f}_i\left(x_t^i\right)=\hat{\mu}_i+\hat{\sigma}_i.
              \Phi^{-1}\left(\hat{F}_i\left(x_t^i\right)\right)$.
    \item Nalezení Grangerovy kauzality v rámci $\hat{f}_i\left(x_t^i\right)$.
\end{enumerate}
přičemž je dále brán v potaz Winsorizovaný\footnote{Winsorizace nebo Winsorova
transformace je transformace statistických dat omezením extrémních hodnot,
aby se snížil vliv případných odlehlých hodnot} odhad použité distribuční
funkce, podle~\cite{Bahadori2013}, aby se zabránilo velkým číslům
$\Phi^{-1}\left(0^{+}\right)$\footnote{$\Phi^{-1}$ je inverzní kumulativní
distribuční funkce standardního normálního rozdělení.} a
$\Phi^{-1}\left(1^{-}\right)$:
\begin{equation}
    \tilde{F}_j= \begin{cases}\delta_n, & \text { if } \hat{F}\left(x^j\right)<\delta_n \\ \hat{F}\left(x^j\right) & \text { if } \delta_n \leq \hat{F}\left(x^j\right)<1-\delta_n \\ \left(1-\delta_n\right) & \text { if } \hat{F}\left(x^j\right)>1-\delta_n .\end{cases}
\end{equation}

\subsection{Metriky hodnocení v strojovém učení}
\label{subsec:ml_metriky}
Pro potřeby porovnání a vyhodnocení výkonnosti (přesnosti) realizovaných modelů
strojového učení bylo zvoleno několik hodnotících metrik. Tato kapitola se
zabývá použitými metrikami v této práci a jejich významem při posuzování modelů.

\subsubsection{Přesnost}
Přesnost je měřítkem, které hodnotí schopnost modelu správně předpovídat
označení tříd v souboru dat. Vypočítá se jako poměr počtu správných předpovědí k
celkovému počtu předpovědí provedených modelem:
\begin{equation}
    \text{Přesnost} = \frac{\text{Počet správně predikovaných tříd}}{\text{Celkový počet predickí}} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}
kde TP, TN, FP a FN představují počet pravdivě pozitivních, pravdivě
negativních, falešně pozitivních a falešně negativních výsledků.

Přesnost je často používanou metrikou pro hodnocení modelů, protože poskytuje
jednoduché měřítko toho, jak dobře model celkově funguje. Vysoká hodnota
přesnosti naznačuje, že model je schopen správně předpovídat velkou část souboru
dat, zatímco nízká hodnota přesnosti naznačuje, že model není příliš efektivní
při predikci tříd. Tato metrika však může být zavádějící ve chvíli, kdy jsou
třídy nevyvážené. V takových případech může mít model, který vždy předpovídá
většinovou třídu, vysokou hodnotu přesnosti, i když si vede špatně u menšinové
třídy.

\subsubsection{Pravdivě pozitivní míra}
Pravdivě pozitivní míra (\gls{TPR}, true positive rate), neboli senzitivita, je
podíl pravdivě pozitivních předpovědí (TP) z celkového počtu pozitivních
předpovědí (TP + falešně negativní předpovědi, tedy TP + FN). Matematicky je
přesnost definována jako:
\begin{equation}
    TPR = \frac{TP}{TP + FN}
\end{equation}

Vysoké \gls{TPR} znamená, že model dobře identifikuje pozitivní případy a
minimalizuje falešně negativní, zatímco nízké \gls{TPR} naznačuje, že model není
příliš účinný při identifikaci pozitivních tříd.

\subsubsection{Pravdivě negativní míra}
Pravdivě negativní míra (\gls{TNR}, true negative rate), neboli specificita,
označuje podíl skutečně negativních případů, které jsou správně identifikovány
jako negativní. Jinými slovy, \gls{TNR} měří schopnost modelu správně
identifikovat negativní třídu a lze vypočítat:
\begin{equation}
    TNR = \frac{TN}{TN + FP}
\end{equation}

Vysoké \gls{TNR} znamená, že model dobře identifikuje negativní třídy a
minimalizuje falešně pozitivní výsledky, zatímco nízké \gls{TNR} naznačuje, že
model není příliš účinný při identifikaci negativních případů.

% \subsubsection{Senzitivita}
% Senzitivita, známá také jako úplnost, hodnotí schopnost modelu správně
% identifikovat pozitivní případy ze všech skutečných pozitivních případů.
% Senzitivita se vypočítá jako poměr pravdivě pozitivních případů (TP) k součtu
% pravdivě pozitivních a falešně negativních případů (FN):
% \begin{equation}
%     \text{Senzitivita} = \frac{TP}{TP + FN}
% \end{equation}

% Vysoká hodnota senzitivity tedy naznačuje, že model je schopen detekovat většinu
% pozitivních tříd a naopak.

% \subsubsection{F1-skóre}
% F1-skóre je harmonický průměr TPR a senzitivity. Matematicky je F1-skóre
% definováno jako:
% \begin{equation}
%     F1 = 2 \times \frac{TPR \times \text{Senzitivita}}{TPR + \text{Senzitivita}}.
% \end{equation}

% Tato metrika je užitečné zejména v případech, kdy jsou třídy nevyvážené, což
% znamená, že existuje výrazně více případů jedné třídy než druhé, protože bere v
% úvahu jak falešně pozitivní, tak falešně negativní výsledky.

\subsubsection{Křivka ROC}
\gls{ROC} (Receiver Operating Characteristic) křivka je grafická metoda pro
vyhodnocení výkonosti binárního klasifikačního modelu. Tato křivka zobrazuje
vztah mezi dvěma parametry: \gls{TPR} a falešnou pozitivní mírou (\gls{FPR},
false positive rate). \gls{TPR} odpovídá přesnosti modelu při detekci
pozitivních případů, zatímco FPR zahrnuje chybně klasifikované negativní
případy.

\gls{ROC} křivka se skládá z mnoha bodů, přičemž každý bod představuje jinou
úroveň rozhodovacího prahu (threshold) pro klasifikaci pozitivních a negativních
případů. \gls{TPR} a \gls{FPR} se poté vypočítávají pro každý z těchto prahů a
vykreslí se na grafu. Ideální křivka ROC je tvořena body s TPR rovnoměrně
rostoucími k jedné a \gls{FPR} rovnoměrně rostoucími k nule, což by znamenalo,
že model dokáže rozlišovat mezi pozitivními a negativními případy s vysokou
přesností. Křivka se používá například k porovnávání výkonu různých
klasifikačních modelů. Vyšší oblast pod křivkou (\gls{AUC}, Area Under Curve)
indikuje, že model má lepší výkon, zatímco nižší \gls{AUC} může naznačovat, že
model má menší přesnost.